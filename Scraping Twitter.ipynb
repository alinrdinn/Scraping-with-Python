{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Twitter\n",
    "Made by Ali Nurdin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will import urllib parse for make Twitter's url based on given keywords\n",
    "import urllib.parse as urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keyword for searching\n",
    "keywords = [\"covid\",\"coronavirus\",\"covid variant\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Twitter's urls based on keywords\n",
    "urls = []\n",
    "for keyword in keywords:\n",
    "    urls.append(\"https://twitter.com/search?q=\"+urlparse.quote_plus(keyword)+\"&src=typed_query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section will import Selenium and Time\n",
    "from selenium import webdriver\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section will open Web Driver\n",
    "Driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap(maximum):\n",
    "    tweets_container = Driver.find_element_by_xpath('/html/body/div/div/div/div[2]/main/div/div/div/div/div/div[2]/div/div/section/div/div')\n",
    "    return_tweets = []\n",
    "    while len(return_tweets) <= maximum:\n",
    "        tweets = tweets_container.find_elements_by_xpath('div')\n",
    "        for tweet in tweets:\n",
    "            try:\n",
    "                article = tweet.find_element_by_tag_name('article')\n",
    "                \n",
    "                # User's name and username\n",
    "                span = article.find_elements_by_tag_name('a')[1].find_elements_by_tag_name('span')\n",
    "                \n",
    "                # Time when tweet was made\n",
    "                time1 = article.find_element_by_tag_name('time').get_attribute('datetime')\n",
    "                \n",
    "                # Tweet\n",
    "                tweet1 = article.find_elements_by_xpath('div/div/div/div[2]/div[2]/div[2]/div')\n",
    "                for i in range(0, len(tweet1)-1):\n",
    "                    tweet1[i] = tweet1[i].text\n",
    "                \n",
    "                # Number of replies, retweets, and likes of the tweet\n",
    "                rep_ret_like = tweet1[-1].get_attribute('aria-label')\n",
    "                \n",
    "                #\n",
    "                data = {\n",
    "                    'name' : span[0].text,\n",
    "                    'username' : span[-1].text,\n",
    "                    'time' : time1,\n",
    "                    'tweet' : ' '.join(tweet1[0:len(tweet1)-2]),\n",
    "                    'num of replay, retweet, and like' : rep_ret_like\n",
    "                }\n",
    "                if not (data in return_tweets):\n",
    "                    return_tweets.append(data)\n",
    "            except:\n",
    "                pass\n",
    "        Driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')\n",
    "        time.sleep(5)\n",
    "    return return_tweets[0:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize array for storing tweets\n",
    "tweets = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "maximum = 30 #maximum tweets per keyword\n",
    "# Scrap all given url\n",
    "for url in urls:\n",
    "    Driver.get(url)\n",
    "    time.sleep(5)\n",
    "    tweets.append(scrap(maximum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing json\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section will enter the data that has been obtained into the json file.\n",
    "file = open(\"Hasil Scraping Twitter.json\", 'w')\n",
    "json.dump(tweets, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
